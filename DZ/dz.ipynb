{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\js\\edu\\2—Å–µ–º–∏–Ω–∞—Ä\\–ú–µ—Ç–æ–¥—ã –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –≤ –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Å–∏—Å—Ç–µ–º–∞—Ö –æ–±—Ä–∞–±–æ—Ç–∫–∏ –Ω–∏—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∏ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è\\–õ–† –∏ –†–ö\\–î–ó\\myenv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.3.1+cu121\n",
      "CUDA available: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "d:\\js\\edu\\2—Å–µ–º–∏–Ω–∞—Ä\\–ú–µ—Ç–æ–¥—ã –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –≤ –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Å–∏—Å—Ç–µ–º–∞—Ö –æ–±—Ä–∞–±–æ—Ç–∫–∏ –Ω–∏—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∏ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è\\–õ–† –∏ –†–ö\\–î–ó\\myenv\\lib\\site-packages\\transformers\\training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "  0%|          | 0/9375 [00:00<?, ?it/s]d:\\js\\edu\\2—Å–µ–º–∏–Ω–∞—Ä\\–ú–µ—Ç–æ–¥—ã –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –≤ –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Å–∏—Å—Ç–µ–º–∞—Ö –æ–±—Ä–∞–±–æ—Ç–∫–∏ –Ω–∏—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∏ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è\\–õ–† –∏ –†–ö\\–î–ó\\myenv\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:435: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      "  5%|‚ñå         | 500/9375 [01:56<36:44,  4.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3878, 'grad_norm': 27.762123107910156, 'learning_rate': 1.8933333333333334e-05, 'epoch': 0.16}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|‚ñà         | 1000/9375 [04:05<35:01,  3.98it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3145, 'grad_norm': 12.772541999816895, 'learning_rate': 1.7866666666666666e-05, 'epoch': 0.32}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|‚ñà‚ñå        | 1500/9375 [06:16<33:43,  3.89it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3037, 'grad_norm': 23.96112632751465, 'learning_rate': 1.6800000000000002e-05, 'epoch': 0.48}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|‚ñà‚ñà‚ñè       | 2000/9375 [08:28<32:20,  3.80it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2793, 'grad_norm': 22.48014259338379, 'learning_rate': 1.5733333333333334e-05, 'epoch': 0.64}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|‚ñà‚ñà‚ñã       | 2500/9375 [10:39<29:19,  3.91it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2595, 'grad_norm': 6.8537373542785645, 'learning_rate': 1.4666666666666666e-05, 'epoch': 0.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|‚ñà‚ñà‚ñà‚ñè      | 3000/9375 [12:51<27:32,  3.86it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2568, 'grad_norm': 36.839447021484375, 'learning_rate': 1.3600000000000002e-05, 'epoch': 0.96}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 33%|‚ñà‚ñà‚ñà‚ñé      | 3126/9375 [17:42<135:00:15, 77.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.23431654274463654, 'eval_runtime': 257.1525, 'eval_samples_per_second': 97.219, 'eval_steps_per_second': 12.152, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|‚ñà‚ñà‚ñà‚ñã      | 3500/9375 [19:22<25:51,  3.79it/s]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1949, 'grad_norm': 0.062145404517650604, 'learning_rate': 1.2533333333333336e-05, 'epoch': 1.12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 4000/9375 [21:36<23:38,  3.79it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.161, 'grad_norm': 38.90977096557617, 'learning_rate': 1.1466666666666668e-05, 'epoch': 1.28}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 4500/9375 [23:49<21:20,  3.81it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1499, 'grad_norm': 0.1815585345029831, 'learning_rate': 1.04e-05, 'epoch': 1.44}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 5000/9375 [26:03<19:25,  3.76it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1565, 'grad_norm': 0.05382585898041725, 'learning_rate': 9.333333333333334e-06, 'epoch': 1.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 5500/9375 [28:17<16:51,  3.83it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1642, 'grad_norm': 10.506442070007324, 'learning_rate': 8.266666666666667e-06, 'epoch': 1.76}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 6000/9375 [30:30<14:25,  3.90it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.149, 'grad_norm': 0.47605034708976746, 'learning_rate': 7.2000000000000005e-06, 'epoch': 1.92}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 6250/9375 [35:51<13:46,  3.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.24927932024002075, 'eval_runtime': 254.5336, 'eval_samples_per_second': 98.219, 'eval_steps_per_second': 12.277, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 6500/9375 [37:02<13:12,  3.63it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1099, 'grad_norm': 0.01797865703701973, 'learning_rate': 6.133333333333334e-06, 'epoch': 2.08}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 7000/9375 [39:07<10:42,  3.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0739, 'grad_norm': 0.08309272676706314, 'learning_rate': 5.0666666666666676e-06, 'epoch': 2.24}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 7500/9375 [41:21<07:53,  3.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0622, 'grad_norm': 63.336181640625, 'learning_rate': 4.000000000000001e-06, 'epoch': 2.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 8000/9375 [43:41<06:32,  3.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0705, 'grad_norm': 0.018638920038938522, 'learning_rate': 2.9333333333333338e-06, 'epoch': 2.56}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 8500/9375 [46:03<03:43,  3.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0759, 'grad_norm': 0.48176220059394836, 'learning_rate': 1.8666666666666669e-06, 'epoch': 2.72}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 9000/9375 [48:18<01:32,  4.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0745, 'grad_norm': 0.9126028418540955, 'learning_rate': 8.000000000000001e-07, 'epoch': 2.88}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9375/9375 [54:34<00:00,  2.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3125171959400177, 'eval_runtime': 271.67, 'eval_samples_per_second': 92.023, 'eval_steps_per_second': 11.503, 'epoch': 3.0}\n",
      "{'train_runtime': 3274.0532, 'train_samples_per_second': 22.907, 'train_steps_per_second': 2.863, 'train_loss': 0.17580392740885417, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3125/3125 [04:24<00:00, 11.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results: {'eval_loss': 0.3125171959400177, 'eval_runtime': 262.777, 'eval_samples_per_second': 95.138, 'eval_steps_per_second': 11.892, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from datasets import load_dataset\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "import torch\n",
    "\n",
    "# Á°ÆËÆ§torchÂÆâË£ÖÊàêÂäü\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "# ÂàõÂª∫ÁªìÊûúÁõÆÂΩïÁöÑÁªùÂØπË∑ØÂæÑ\n",
    "output_dir = 'D:\\\\js\\\\results'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Âä†ËΩΩIMDBÊï∞ÊçÆÈõÜ\n",
    "dataset = load_dataset(\"imdb\")\n",
    "train_dataset = dataset['train']\n",
    "test_dataset = dataset['test']\n",
    "\n",
    "# Âä†ËΩΩÈ¢ÑËÆ≠ÁªÉÁöÑBERTÂàÜËØçÂô®\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# ÂØπÊï∞ÊçÆËøõË°åÁºñÁ†Å\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], padding=\"max_length\", truncation=True)\n",
    "\n",
    "encoded_train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "encoded_test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Âä†ËΩΩÈ¢ÑËÆ≠ÁªÉÁöÑBERTÊ®°Âûã\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "\n",
    "# ÂÆö‰πâËÆ≠ÁªÉÂèÇÊï∞\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "# ÂàõÂª∫TrainerÂÆû‰æã\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=encoded_train_dataset,\n",
    "    eval_dataset=encoded_test_dataset,\n",
    ")\n",
    "\n",
    "# ËÆ≠ÁªÉÊ®°Âûã\n",
    "trainer.train()\n",
    "\n",
    "# ËØÑ‰º∞Ê®°Âûã\n",
    "results = trainer.evaluate()\n",
    "print(\"Evaluation results:\", results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3125/3125 [04:06<00:00, 12.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9412\n",
      "neg             precision: 0.9399 recall: 0.9426 f1: 0.9412 support: 12500\n",
      "pos             precision: 0.9424 recall: 0.9398 f1: 0.9411 support: 12500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Ëé∑ÂèñÈ¢ÑÊµãÁªìÊûú\n",
    "predictions = trainer.predict(encoded_test_dataset)\n",
    "y_pred = np.argmax(predictions.predictions, axis=-1)\n",
    "y_true = predictions.label_ids\n",
    "\n",
    "# ËÆ°ÁÆóÂáÜÁ°ÆÂ∫¶\n",
    "accuracy = np.sum(y_pred == y_true) / len(y_true)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# ÁîüÊàêÂàÜÁ±ªÊä•Âëä\n",
    "def classification_report(y_true, y_pred, target_names):\n",
    "    from collections import defaultdict\n",
    "\n",
    "    report = defaultdict(lambda: defaultdict(int))\n",
    "    for true, pred in zip(y_true, y_pred):\n",
    "        report[true]['total'] += 1\n",
    "        report[pred]['predicted'] += 1\n",
    "        if true == pred:\n",
    "            report[true]['correct'] += 1\n",
    "\n",
    "    output = []\n",
    "    for i, name in enumerate(target_names):\n",
    "        total = report[i]['total']\n",
    "        correct = report[i]['correct']\n",
    "        predicted = report[i]['predicted']\n",
    "        precision = correct / predicted if predicted > 0 else 0\n",
    "        recall = correct / total if total > 0 else 0\n",
    "        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        output.append(f\"{name:15} precision: {precision:.4f} recall: {recall:.4f} f1: {f1:.4f} support: {total}\")\n",
    "\n",
    "    return \"\\n\".join(output)\n",
    "\n",
    "# Ëé∑ÂèñÊ†áÁ≠æÂêçÁß∞\n",
    "label_names = dataset['train'].features['label'].names\n",
    "\n",
    "# ÊâìÂç∞ÂàÜÁ±ªÊä•Âëä\n",
    "report = classification_report(y_true, y_pred, label_names)\n",
    "print(report)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
