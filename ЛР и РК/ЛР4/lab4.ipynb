{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Стратегия:\n",
      "array([[0.25, 0.25, 0.25, 0.25],\n",
      "       [0.25, 0.25, 0.25, 0.25],\n",
      "       [0.25, 0.25, 0.25, 0.25],\n",
      "       [0.25, 0.25, 0.25, 0.25],\n",
      "       [0.25, 0.25, 0.25, 0.25],\n",
      "       [0.25, 0.25, 0.25, 0.25],\n",
      "       [0.25, 0.25, 0.25, 0.25],\n",
      "       [0.25, 0.25, 0.25, 0.25],\n",
      "       [0.25, 0.25, 0.25, 0.25],\n",
      "       [0.25, 0.25, 0.25, 0.25],\n",
      "       [0.25, 0.25, 0.25, 0.25],\n",
      "       [0.25, 0.25, 0.25, 0.25],\n",
      "       [0.25, 0.25, 0.25, 0.25],\n",
      "       [0.25, 0.25, 0.25, 0.25],\n",
      "       [0.25, 0.25, 0.25, 0.25],\n",
      "       [0.25, 0.25, 0.25, 0.25],\n",
      "       [0.25, 0.25, 0.25, 0.25],\n",
      "       [0.25, 0.25, 0.25, 0.25],\n",
      "       [0.25, 0.25, 0.25, 0.25],\n",
      "       [0.25, 0.25, 0.25, 0.25],\n",
      "       [0.25, 0.25, 0.25, 0.25],\n",
      "       [0.25, 0.25, 0.25, 0.25],\n",
      "       [0.25, 0.25, 0.25, 0.25],\n",
      "       [0.25, 0.25, 0.25, 0.25],\n",
      "       [0.25, 0.25, 0.25, 0.25],\n",
      "       [0.25, 0.25, 0.25, 0.25],\n",
      "       [0.25, 0.25, 0.25, 0.25],\n",
      "       [0.25, 0.25, 0.25, 0.25],\n",
      "       [0.25, 0.25, 0.25, 0.25],\n",
      "       [0.25, 0.25, 0.25, 0.25],\n",
      "       [0.25, 0.25, 0.25, 0.25],\n",
      "       [0.25, 0.25, 0.25, 0.25],\n",
      "       [0.25, 0.25, 0.25, 0.25],\n",
      "       [0.25, 0.25, 0.25, 0.25],\n",
      "       [0.25, 0.25, 0.25, 0.25],\n",
      "       [0.25, 0.25, 0.25, 0.25],\n",
      "       [0.25, 0.25, 0.25, 0.25],\n",
      "       [0.25, 0.25, 0.25, 0.25],\n",
      "       [0.25, 0.25, 0.25, 0.25],\n",
      "       [0.25, 0.25, 0.25, 0.25],\n",
      "       [0.25, 0.25, 0.25, 0.25],\n",
      "       [0.25, 0.25, 0.25, 0.25],\n",
      "       [0.25, 0.25, 0.25, 0.25],\n",
      "       [0.25, 0.25, 0.25, 0.25],\n",
      "       [0.25, 0.25, 0.25, 0.25],\n",
      "       [0.25, 0.25, 0.25, 0.25],\n",
      "       [0.25, 0.25, 0.25, 0.25],\n",
      "       [0.25, 0.25, 0.25, 0.25]])\n",
      "Алгоритм выполнился за 1000 шагов.\n",
      "Стратегия:\n",
      "array([[0.        , 0.5       , 0.5       , 0.        ],\n",
      "       [0.33333333, 0.33333333, 0.33333333, 0.        ],\n",
      "       [0.        , 0.        , 1.        , 0.        ],\n",
      "       [0.        , 0.        , 1.        , 0.        ],\n",
      "       [0.        , 0.        , 1.        , 0.        ],\n",
      "       [0.        , 0.        , 1.        , 0.        ],\n",
      "       [0.        , 0.        , 1.        , 0.        ],\n",
      "       [0.        , 0.        , 1.        , 0.        ],\n",
      "       [0.        , 0.        , 1.        , 0.        ],\n",
      "       [0.        , 0.        , 1.        , 0.        ],\n",
      "       [0.33333333, 0.        , 0.33333333, 0.33333333],\n",
      "       [0.        , 0.        , 0.5       , 0.5       ],\n",
      "       [0.        , 0.        , 1.        , 0.        ],\n",
      "       [0.        , 0.5       , 0.5       , 0.        ],\n",
      "       [0.        , 0.5       , 0.5       , 0.        ],\n",
      "       [0.        , 0.33333333, 0.33333333, 0.33333333],\n",
      "       [0.        , 0.33333333, 0.33333333, 0.33333333],\n",
      "       [0.        , 0.33333333, 0.33333333, 0.33333333],\n",
      "       [0.        , 0.33333333, 0.33333333, 0.33333333],\n",
      "       [0.        , 0.33333333, 0.33333333, 0.33333333],\n",
      "       [0.        , 0.33333333, 0.33333333, 0.33333333],\n",
      "       [0.        , 0.        , 0.5       , 0.5       ],\n",
      "       [0.        , 0.        , 0.5       , 0.5       ],\n",
      "       [0.        , 0.        , 1.        , 0.        ],\n",
      "       [0.        , 0.33333333, 0.33333333, 0.33333333],\n",
      "       [0.        , 0.5       , 0.        , 0.5       ],\n",
      "       [0.33333333, 0.33333333, 0.        , 0.33333333],\n",
      "       [0.33333333, 0.33333333, 0.        , 0.33333333],\n",
      "       [0.33333333, 0.33333333, 0.        , 0.33333333],\n",
      "       [0.33333333, 0.33333333, 0.        , 0.33333333],\n",
      "       [0.33333333, 0.33333333, 0.        , 0.33333333],\n",
      "       [0.33333333, 0.33333333, 0.        , 0.33333333],\n",
      "       [0.33333333, 0.33333333, 0.        , 0.33333333],\n",
      "       [0.33333333, 0.33333333, 0.        , 0.33333333],\n",
      "       [0.        , 0.5       , 0.        , 0.5       ],\n",
      "       [0.        , 0.33333333, 0.33333333, 0.33333333],\n",
      "       [0.33333333, 0.        , 0.33333333, 0.33333333],\n",
      "       [0.5       , 0.        , 0.        , 0.5       ],\n",
      "       [1.        , 0.        , 0.        , 0.        ],\n",
      "       [1.        , 0.        , 0.        , 0.        ],\n",
      "       [1.        , 0.        , 0.        , 0.        ],\n",
      "       [1.        , 0.        , 0.        , 0.        ],\n",
      "       [1.        , 0.        , 0.        , 0.        ],\n",
      "       [1.        , 0.        , 0.        , 0.        ],\n",
      "       [1.        , 0.        , 0.        , 0.        ],\n",
      "       [1.        , 0.        , 0.        , 0.        ],\n",
      "       [0.5       , 0.5       , 0.        , 0.        ],\n",
      "       [0.33333333, 0.33333333, 0.33333333, 0.        ]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Programs\\python\\bin\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pprint import pprint\n",
    "\n",
    "\n",
    "class PolicyIterationAgent:\n",
    "    '''\n",
    "    Класс, эмулирующий работу агента\n",
    "    '''\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "        # Пространство состояний\n",
    "        self.observation_dim = env.observation_space.n\n",
    "        # Массив действий в соответствии с документацией\n",
    "        self.actions_variants = np.arange(env.action_space.n)\n",
    "        # Задание стратегии (политики)\n",
    "        self.policy_probs = np.full((self.observation_dim, len(self.actions_variants)), 1 / len(self.actions_variants))\n",
    "        # Начальные значения для v(s)\n",
    "        self.state_values = np.zeros(shape=(self.observation_dim))\n",
    "        # Начальные значения параметров\n",
    "        self.maxNumberOfIterations = 1000\n",
    "        self.theta = 1e-6\n",
    "        self.gamma = 0.99\n",
    "\n",
    "    def print_policy(self):\n",
    "        '''\n",
    "        Вывод матриц стратегии\n",
    "        '''\n",
    "        print('Стратегия:')\n",
    "        pprint(self.policy_probs)\n",
    "\n",
    "    def policy_evaluation(self):\n",
    "        '''\n",
    "        Оценивание стратегии \n",
    "        '''\n",
    "        # Предыдущее значение функции ценности\n",
    "        valueFunctionVector = self.state_values\n",
    "        for iterations in range(self.maxNumberOfIterations):\n",
    "            # Новое значение функции ценности\n",
    "            valueFunctionVectorNextIteration = np.zeros(shape=(self.observation_dim))\n",
    "            # Цикл по состояниям\n",
    "            for state in range(self.observation_dim):\n",
    "                # Вероятности действий\n",
    "                action_probabilities = self.policy_probs[state]\n",
    "                # Цикл по действиям\n",
    "                outerSum = 0\n",
    "                for action, prob in enumerate(action_probabilities):\n",
    "                    innerSum = 0\n",
    "                    # Цикл по вероятностям действий\n",
    "                    for probability, next_state, reward, isTerminalState in self.env.P[state][action]:\n",
    "                        innerSum += probability * (reward + self.gamma * self.state_values[next_state])\n",
    "                    outerSum += self.policy_probs[state][action] * innerSum\n",
    "                valueFunctionVectorNextIteration[state] = outerSum\n",
    "            if np.max(np.abs(valueFunctionVectorNextIteration - valueFunctionVector)) < self.theta:\n",
    "                # Проверка сходимости алгоритма\n",
    "                valueFunctionVector = valueFunctionVectorNextIteration\n",
    "                break\n",
    "            valueFunctionVector = valueFunctionVectorNextIteration\n",
    "        return valueFunctionVector\n",
    "\n",
    "    def policy_improvement(self):\n",
    "        '''\n",
    "        Улучшение стратегии \n",
    "        '''\n",
    "        qvaluesMatrix = np.zeros((self.observation_dim, len(self.actions_variants)))\n",
    "        improvedPolicy = np.zeros((self.observation_dim, len(self.actions_variants)))\n",
    "        # Цикл по состояниям\n",
    "        for state in range(self.observation_dim):\n",
    "            for action in range(len(self.actions_variants)):\n",
    "                for probability, next_state, reward, isTerminalState in self.env.P[state][action]:\n",
    "                    qvaluesMatrix[state, action] += probability * (reward + self.gamma * self.state_values[next_state])\n",
    "\n",
    "            # Находим лучшие индексы\n",
    "            bestActionIndex = np.where(qvaluesMatrix[state, :] == np.max(qvaluesMatrix[state, :]))\n",
    "            # Обновление стратегии\n",
    "            improvedPolicy[state, bestActionIndex] = 1 / np.size(bestActionIndex)\n",
    "        return improvedPolicy\n",
    "\n",
    "    def policy_iteration(self, cnt):\n",
    "        '''\n",
    "        Основная реализация алгоритма\n",
    "        '''\n",
    "        policy_stable = False\n",
    "        for i in range(1, cnt + 1):\n",
    "            self.state_values = self.policy_evaluation()\n",
    "            self.policy_probs = self.policy_improvement()\n",
    "        print(f'Алгоритм выполнился за {i} шагов.')\n",
    "\n",
    "\n",
    "def play_agent(agent):\n",
    "    env2 = gym.make('CliffWalking-v0', render_mode='human')\n",
    "    state = env2.reset()[0]\n",
    "    done = False\n",
    "    while not done:\n",
    "        p = agent.policy_probs[state]\n",
    "        if isinstance(p, np.ndarray):\n",
    "            action = np.random.choice(len(agent.actions_variants), p=p)\n",
    "        else:\n",
    "            action = p\n",
    "        next_state, reward, terminated, truncated, _ = env2.step(action)\n",
    "        env2.render()\n",
    "        state = next_state\n",
    "        if terminated or truncated:\n",
    "            done = True\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Создание среды\n",
    "    env = gym.make('CliffWalking-v0')\n",
    "    env.reset()\n",
    "    # Обучение агента\n",
    "    agent = PolicyIterationAgent(env)\n",
    "    agent.print_policy()\n",
    "    agent.policy_iteration(1000)\n",
    "    agent.print_policy()\n",
    "    # Проигрывание сцены для обученного агента\n",
    "    play_agent(agent)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
